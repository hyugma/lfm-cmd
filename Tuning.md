# lfm-cmd Tuning & Optimization History

本ドキュメントでは、`lfm-cmd` の開発過程で行われた主要なハイパーパラメータの調整、バグフィックス、および自然な日本語出力を得るための最適化プロセスを記録します。

## 1. チャンク分割とシステムプロンプトの調整
初期の実装では、モデルの安定性を確保するために、テキストのチャンクサイズやシステムプロンプトの調整を行いました。
- **動的チャンク分割**: 単純な文字数ではなく、LlamaModel のトークナイザーを用いた「トークン数ベース」のチャンク分割（`chunker.rs`）を実装。文の区切り（`\n`, `。`, `.`）を考慮することで、文脈の分断を防ぎました。
- **メタプロンプティング**: 先頭のテキストからジャンルや文体を推測し、後続の中間要約や最終要約に適用する「動的システムプロンプト（Meta-Prompt）」を導入。これにより、要約の精度と一貫性が向上しました。

## 2. 語彙欠落（ゲシュタルト崩壊）問題の解決
頻出する名詞（「坊っちゃん」「小日向」など）の先頭文字が欠落する現象が発生しました。これは以下の複数の要因が絡み合っていました。

### 2.1. 履歴バッファの分離 (History Buffer Isolation)
- **原因**: 入力プロンプト（元のテキスト全体）のトークンが `sampler.accept_many(&tokens)` によって反復ペナルティ（Repetition Penalty）の履歴バッファに登録されていました。これにより、プロンプト内に出現する単語をAIが生成しようとすると、即座に重いペナルティが課され、出力が阻害されていました。
- **解決策**: `sampler.accept_many` の呼び出しを削除し、ペナルティの対象を「AIが新たに生成したトークンのみ」に限定しました。

### 2.2. ペナルティ値とウィンドウサイズの最適化
- **ウィンドウサイズの縮小**: 日本語のような膠着語では、助詞（て、に、を、は）や語尾（〜ます、〜た）が自然に反復します。評価ウィンドウ（`penalty_last_n`）が広すぎるとこれらが抑制され、電報のようなカタコトの文章になってしまいます。ウィンドウサイズを `128` → `64` → **`32`** へと大幅に縮小し、直近の異常な無限ループのみを抑制するようにしました。
- **ペナルティ値の緩和**: 基本のペナルティ強度を `1.15` から **`1.10`** に緩和しました。

### 2.3. UTF-8 マルチバイト文字のデコードバグの修正 (Native Decode Patch)
- **原因**: 履歴バッファ分離後も「坊」などの特定の漢字が欠落する現象が継続しました。調査の結果、`llama-cpp-2` のネイティブバインディング `token_to_piece` に潜むサイレントバグが原因と判明しました。
- **メカニズム**: 「ちゃん」（9バイト）などの長いBPEマルチバイトトークンが生成された際、独自に実装した `decode_token` ラッパー関数において `token_to_piece_bytes` へ渡す初期バッファサイズが `8` バイトにハードコードされていました。このため、9バイト以上のトークンを受信すると `InsufficientBufferSpace` エラーが発生して握り潰され、該当トークンが丸ごと消失（「坊っちゃん」→「坊っ」）していました。
- **解決策**: `src/types.rs` 内の `token_to_piece_bytes` の要求バッファサイズを `32` バイトに拡大することで、長い日本語トークンもエラーなく完全に取得・デコードできるようになり、語彙の消失問題が完全に解決しました。

## 3. 完全な自然言語サンプリングへの移行 (Transition to Natural Language Sampling)
長文を統合する Reduce フェーズや、初期化の段階において、Repetition Penalty が日本語のひらがな（助詞や語尾）を過剰に抑制し、不自然な文体（「坊っちゃん」が「坊っ」になる現象など）を引き起こすことが判明しました。

- **Repetition Penalty の完全無効化**: Worker（Map）、Prompts（メタプロンプト）、Reducer（中間・最終要約）のすべての生成プロセスにおいて、ペナルティパラメーターを `1.00`（無効）に設定しました。
- **サンプリングの多様性確保**: ペナルティを解除する代わりに、`temperature` を `0.1` → **`0.2`** に引き上げ、`top_p` を `0.1` → **`0.9`** に拡張しました。これにより人工的な文法抑制を排除しつつ、言語モデルが元来持つ分布確率による自然な表現の揺らぎ（多様性）を担保しています。

---

以上のチューニングにより、`lfm-cmd` はコンテキストの欠落を防ぎつつ、長文に対しても文法的に自然で流暢な日本語要約を生成できるようになりました。
